{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mz-4_k44O4r6"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "import time as time\n",
    "from PIL import ImageFile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2184,
     "status": "ok",
     "timestamp": 1582463684475,
     "user": {
      "displayName": "murali perumalla",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mC9CEkZ2SGMnNSmTjQEQaKRd0BHAnwhE1GCElJY=s64",
      "userId": "05154681292036637401"
     },
     "user_tz": -330
    },
    "id": "WJCihg9oWkOc",
    "outputId": "5d6c7ec9-f563-46e4-aa25-065aa33d8d41"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this project is done in colab <br>\n",
    "Datset is preprocessed according to the need and upload to the drive <br>\n",
    "intial structure of dataset is <br>\n",
    "Breakhis/ ,<br>\n",
    "&emsp; benign/ <br>\n",
    "&emsp; &emsp;class0/images.. <br>\n",
    "&emsp; &emsp;class1/images.. <br>\n",
    "&emsp; &emsp;class2/images.. <br>\n",
    "&emsp; &emsp;class3/images.. <br>\n",
    "&emsp; malignant/ <br>\n",
    "&emsp; &emsp;class4/images.. <br>\n",
    "&emsp; &emsp;class5/images.. <br>\n",
    "&emsp; &emsp;class6/images.. <br>\n",
    "&emsp; &emsp;class7/images.. <br>\n",
    "\n",
    "\n",
    "After running the pred_data.py , dataset structure is <br>\n",
    "Breakhis/ ,<br>\n",
    "&emsp; train/ <br>\n",
    "&emsp; &emsp;class0/images.. <br>\n",
    "&emsp; &emsp;class1/images.. <br>\n",
    "&emsp; &emsp;class2/images.. <br>\n",
    "&emsp; &emsp;class3/images.. <br>\n",
    "&emsp; &emsp;class4/images.. <br>\n",
    "&emsp; &emsp;class5/images.. <br>\n",
    "&emsp; &emsp;class6/images.. <br>\n",
    "&emsp; &emsp;class7/images.. <br>\n",
    "&emsp; test/ <br>\n",
    "&emsp; &emsp;class0/images.. <br>\n",
    "&emsp; &emsp;class1/images.. <br>\n",
    "&emsp; &emsp;class2/images.. <br>\n",
    "&emsp; &emsp;class3/images.. <br>\n",
    "&emsp; &emsp;class4/images.. <br>\n",
    "&emsp; &emsp;class5/images.. <br>\n",
    "&emsp; &emsp;class6/images.. <br>\n",
    "&emsp; &emsp;class7/images.. <br>\n",
    "&emsp; valid/ <br>\n",
    "&emsp; &emsp;class0/images.. <br>\n",
    "&emsp; &emsp;class1/images.. <br>\n",
    "&emsp; &emsp;class2/images.. <br>\n",
    "&emsp; &emsp;class3/images.. <br>\n",
    "&emsp; &emsp;class4/images.. <br>\n",
    "&emsp; &emsp;class5/images.. <br>\n",
    "&emsp; &emsp;class6/images.. <br>\n",
    "&emsp; &emsp;class7/images.. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1939,
     "status": "ok",
     "timestamp": 1582463682443,
     "user": {
      "displayName": "murali perumalla",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mC9CEkZ2SGMnNSmTjQEQaKRd0BHAnwhE1GCElJY=s64",
      "userId": "05154681292036637401"
     },
     "user_tz": -330
    },
    "id": "MW0UcWZDO9-w",
    "outputId": "af5d8be1-34e8-48b7-b948-8b558a8ad16a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "#mount drive in colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 72547,
     "status": "ok",
     "timestamp": 1582452180272,
     "user": {
      "displayName": "murali perumalla",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mC9CEkZ2SGMnNSmTjQEQaKRd0BHAnwhE1GCElJY=s64",
      "userId": "05154681292036637401"
     },
     "user_tz": -330
    },
    "id": "_CnDF6XDPeoA",
    "outputId": "fb2d9fa2-7e6f-4464-9def-7b7b092f0c2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /content/drive/My Drive/prep_data1.zip\n",
      "replace prep_data1/test/A/SOB_B_A-14-22549AB-100-001.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
      "replace prep_data1/test/A/SOB_B_A-14-22549AB-100-009.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
     ]
    }
   ],
   "source": [
    "!unzip /content/drive/'My Drive'/prep_data1.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2261,
     "status": "ok",
     "timestamp": 1582463688864,
     "user": {
      "displayName": "murali perumalla",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mC9CEkZ2SGMnNSmTjQEQaKRd0BHAnwhE1GCElJY=s64",
      "userId": "05154681292036637401"
     },
     "user_tz": -330
    },
    "id": "_yElWMqqO4sA",
    "outputId": "8707501b-1a70-4da7-ea90-f3c989a8ca92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'DC', 'F', 'LC', 'MC', 'PC', 'PT', 'TA']\n"
     ]
    }
   ],
   "source": [
    "#path\n",
    "data_dir = '/content/prep_data1/'\n",
    "\n",
    "# Define transforms for the training data and testing data\n",
    "train_transforms=transforms.Compose([transforms.Resize(256),\n",
    "                                    transforms.RandomHorizontalFlip(),\n",
    "                                    transforms.RandomVerticalFlip(),\n",
    "                                    transforms.RandomRotation(15),\n",
    "                                    transforms.CenterCrop(224),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize([0.5, 0.5, 0.5],\n",
    "                                                        [0.5, 0.5, 0.5])\n",
    "                                   ])\n",
    "#transformations for validation and test  datasets\n",
    "valid_transforms=transforms.Compose([transforms.Resize(256),\n",
    "                                     transforms.CenterCrop(224),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize([0.5, 0.5, 0.5],\n",
    "                                                        [0.5, 0.5, 0.5])\n",
    "                                    ])\n",
    "\n",
    "\n",
    "train_data=datasets.ImageFolder(os.path.join(data_dir,'train'),transform=train_transforms)\n",
    "valid_data=datasets.ImageFolder(os.path.join(data_dir,'valid'),transform=valid_transforms)\n",
    "test_data=datasets.ImageFolder(os.path.join(data_dir,'test'),transform=valid_transforms)\n",
    "classes=train_data.classes\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of samples in each class [887, 6130, 480, 317, 972, 101, 128] <br>\n",
    "dataset is Imbalanced <br>\n",
    "in order to deal with imbalance data set i am using WeightedRandomSampler() from pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9B-7VyYsO4sG"
   },
   "outputs": [],
   "source": [
    "#count the number of samples in each image\n",
    "sample_counts=[] \n",
    "for i in [train_data.imgs,test_data.imgs,valid_data.imgs]:\n",
    "    for item in i:\n",
    "        sample_counts.append(item[1])\n",
    "class_counts=dict(pd.Series(sample_counts).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GwJAFj3mO4sL",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Get weights to each and images\n",
    "class_sample_count=[class_counts[i] for i in sorted(class_counts)]\n",
    "weights = 1 / torch.Tensor(class_sample_count)\n",
    "weights = weights.double()\n",
    "train_targets = [sample[1] for sample in train_data.imgs]\n",
    "train_samples_weight = [weights[class_id] for class_id in train_targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jObpTs24O4sQ",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#from torchsampler import ImbalancedDatasetSampler\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "batch_size=64\n",
    "\n",
    "#loding the data into loaders\n",
    "train_Loader=torch.utils.data.DataLoader(train_data,batch_size=batch_size,sampler=WeightedRandomSampler(train_samples_weight, len(train_data)),shuffle=False,num_workers=4)\n",
    "valid_Loader=torch.utils.data.DataLoader(valid_data,batch_size=batch_size,shuffle=False,num_workers=4)\n",
    "test_Loader=torch.utils.data.DataLoader(test_data,batch_size=batch_size,shuffle=False,num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# small dataset , different data -transfer learning\n",
    "In our case dataset is small, so todeal with small datasets is a good to remove the high level feature layers from pretrained model. <br>\n",
    "\n",
    "In this project i have used vgg16(),<br>\n",
    "## fine-tune\n",
    "removed the last convolutional block of vgg16 i.e last 3convolutional layers followed by action and maxpooling\n",
    "and add classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 638
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 21174,
     "status": "ok",
     "timestamp": 1582463719773,
     "user": {
      "displayName": "murali perumalla",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mC9CEkZ2SGMnNSmTjQEQaKRd0BHAnwhE1GCElJY=s64",
      "userId": "05154681292036637401"
     },
     "user_tz": -330
    },
    "id": "DifZSZRcO4sU",
    "outputId": "4212cd5b-a64e-446b-88a8-3cf80ca97435"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vgg_net(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=100352, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=8, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class vgg_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        vgg_model=models.vgg16(pretrained=True)\n",
    "        self.features=nn.Sequential(*list(vgg_model.features)[:-7])\n",
    "        self.classifier=nn.Sequential(nn.Linear(100352,4096),*list(vgg_model.classifier[1:-1]),nn.Linear(4096,8))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=self.features(x)\n",
    "        x = x.view(-1, 512 * 14 * 14)\n",
    "        x=self.classifier(x)\n",
    "        \n",
    "        return x\n",
    "model=vgg_net()\n",
    "model        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bTO17NRRO4sb"
   },
   "outputs": [],
   "source": [
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "#model.classifier[0].in_features=100352\n",
    "if use_cuda:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mention the criterion and optimizers and their parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t-OmPBHlO4sk"
   },
   "outputs": [],
   "source": [
    "#mention the criterion and optimizers and their parameters\n",
    "from torch.optim.lr_scheduler import StepLR,ReduceLROnPlateau\n",
    "criterion= nn.CrossEntropyLoss()\n",
    "optimizer= optim.SGD(model.parameters(),lr=0.007,momentum=0.9,weight_decay=0.1, nesterov=True)\n",
    "#scheduler = StepLR(optimizer_transfer, step_size=2, gamma=0.2)\n",
    "scheduler=ReduceLROnPlateau(optimizer, mode='max', factor=0.7, patience=1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i-7xN0cQy1IV"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train and save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4652775,
     "status": "ok",
     "timestamp": 1582468505736,
     "user": {
      "displayName": "murali perumalla",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mC9CEkZ2SGMnNSmTjQEQaKRd0BHAnwhE1GCElJY=s64",
      "userId": "05154681292036637401"
     },
     "user_tz": -330
    },
    "id": "cURcNsyTO4sp",
    "outputId": "73ff5001-848a-4c57-8a5a-9b3918f36fe6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  Training Loss: 1.892627  Accuracy 0.241089  Validation Loss: 1.730102\n",
      "Validation loss decreased (inf --> 1.730102). Saving model...\n",
      "Epoch: 2  Training Loss: 1.318676  Accuracy 0.498156  Validation Loss: 1.279369\n",
      "Validation loss decreased (1.730102 --> 1.279369). Saving model...\n",
      "Epoch: 3  Training Loss: 1.079068  Accuracy 0.604741  Validation Loss: 1.157873\n",
      "Epoch     3: reducing learning rate of group 0 to 4.9000e-03.\n",
      "Validation loss decreased (1.279369 --> 1.157873). Saving model...\n",
      "Epoch: 4  Training Loss: 0.908948  Accuracy 0.668306  Validation Loss: 1.190614\n",
      "Epoch: 5  Training Loss: 0.870743  Accuracy 0.687270  Validation Loss: 1.298284\n",
      "Epoch     5: reducing learning rate of group 0 to 3.4300e-03.\n",
      "Epoch: 6  Training Loss: 0.758732  Accuracy 0.733802  Validation Loss: 1.059741\n",
      "Validation loss decreased (1.157873 --> 1.059741). Saving model...\n",
      "Epoch: 7  Training Loss: 0.703472  Accuracy 0.758736  Validation Loss: 0.964628\n",
      "Epoch     7: reducing learning rate of group 0 to 2.4010e-03.\n",
      "Validation loss decreased (1.059741 --> 0.964628). Saving model...\n",
      "Epoch: 8  Training Loss: 0.642328  Accuracy 0.783143  Validation Loss: 0.981449\n",
      "Epoch: 9  Training Loss: 0.617529  Accuracy 0.798771  Validation Loss: 0.982988\n",
      "Epoch     9: reducing learning rate of group 0 to 1.6807e-03.\n",
      "Epoch: 10  Training Loss: 0.565079  Accuracy 0.815277  Validation Loss: 0.868628\n",
      "Validation loss decreased (0.964628 --> 0.868628). Saving model...\n",
      "Epoch: 11  Training Loss: 0.549147  Accuracy 0.824583  Validation Loss: 0.912519\n",
      "Epoch    11: reducing learning rate of group 0 to 1.1765e-03.\n",
      "Epoch: 12  Training Loss: 0.505577  Accuracy 0.845654  Validation Loss: 0.828892\n",
      "Validation loss decreased (0.868628 --> 0.828892). Saving model...\n",
      "Epoch: 13  Training Loss: 0.492595  Accuracy 0.854083  Validation Loss: 0.800889\n",
      "Epoch    13: reducing learning rate of group 0 to 8.2354e-04.\n",
      "Validation loss decreased (0.828892 --> 0.800889). Saving model...\n",
      "Epoch: 14  Training Loss: 0.465878  Accuracy 0.860053  Validation Loss: 0.815650\n",
      "Epoch: 15  Training Loss: 0.447040  Accuracy 0.877963  Validation Loss: 0.846567\n",
      "Epoch    15: reducing learning rate of group 0 to 5.7648e-04.\n",
      "Epoch: 16  Training Loss: 0.429589  Accuracy 0.890430  Validation Loss: 0.804578\n",
      "Epoch: 17  Training Loss: 0.422777  Accuracy 0.891308  Validation Loss: 0.819475\n",
      "Epoch    17: reducing learning rate of group 0 to 4.0354e-04.\n",
      "Epoch: 18  Training Loss: 0.419501  Accuracy 0.889728  Validation Loss: 0.813533\n",
      "Epoch: 19  Training Loss: 0.404051  Accuracy 0.898507  Validation Loss: 0.812800\n",
      "Epoch    19: reducing learning rate of group 0 to 2.8248e-04.\n",
      "Epoch: 20  Training Loss: 0.386105  Accuracy 0.904829  Validation Loss: 0.777789\n",
      "Validation loss decreased (0.800889 --> 0.777789). Saving model...\n",
      "Epoch: 21  Training Loss: 0.397094  Accuracy 0.902897  Validation Loss: 0.800741\n",
      "Epoch    21: reducing learning rate of group 0 to 1.9773e-04.\n",
      "Epoch: 22  Training Loss: 0.384719  Accuracy 0.904829  Validation Loss: 0.777940\n",
      "Epoch: 23  Training Loss: 0.374619  Accuracy 0.914311  Validation Loss: 0.795216\n",
      "Epoch    23: reducing learning rate of group 0 to 1.3841e-04.\n",
      "Epoch: 24  Training Loss: 0.384732  Accuracy 0.909043  Validation Loss: 0.775592\n",
      "Validation loss decreased (0.777789 --> 0.775592). Saving model...\n",
      "Epoch: 25  Training Loss: 0.382013  Accuracy 0.914135  Validation Loss: 0.775732\n",
      "Epoch    25: reducing learning rate of group 0 to 9.6889e-05.\n",
      "Epoch: 26  Training Loss: 0.351113  Accuracy 0.925724  Validation Loss: 0.778562\n",
      "Epoch: 27  Training Loss: 0.369875  Accuracy 0.911677  Validation Loss: 0.771938\n",
      "Epoch    27: reducing learning rate of group 0 to 6.7822e-05.\n",
      "Validation loss decreased (0.775592 --> 0.771938). Saving model...\n",
      "Epoch: 28  Training Loss: 0.376307  Accuracy 0.912730  Validation Loss: 0.769437\n",
      "Validation loss decreased (0.771938 --> 0.769437). Saving model...\n",
      "Epoch: 29  Training Loss: 0.362255  Accuracy 0.922212  Validation Loss: 0.790173\n",
      "Epoch    29: reducing learning rate of group 0 to 4.7476e-05.\n",
      "Epoch: 30  Training Loss: 0.374042  Accuracy 0.916945  Validation Loss: 0.765334\n",
      "Validation loss decreased (0.769437 --> 0.765334). Saving model...\n",
      "Epoch: 31  Training Loss: 0.354378  Accuracy 0.922915  Validation Loss: 0.767017\n",
      "Epoch    31: reducing learning rate of group 0 to 3.3233e-05.\n",
      "Epoch: 32  Training Loss: 0.358864  Accuracy 0.922915  Validation Loss: 0.775971\n",
      "Epoch: 33  Training Loss: 0.370755  Accuracy 0.915013  Validation Loss: 0.770269\n",
      "Epoch    33: reducing learning rate of group 0 to 2.3263e-05.\n",
      "Epoch: 34  Training Loss: 0.367590  Accuracy 0.917296  Validation Loss: 0.764806\n",
      "Validation loss decreased (0.765334 --> 0.764806). Saving model...\n",
      "Epoch: 35  Training Loss: 0.357806  Accuracy 0.926075  Validation Loss: 0.766548\n",
      "Epoch    35: reducing learning rate of group 0 to 1.6284e-05.\n",
      "Epoch: 36  Training Loss: 0.356755  Accuracy 0.923968  Validation Loss: 0.763093\n",
      "Validation loss decreased (0.764806 --> 0.763093). Saving model...\n",
      "Epoch: 37  Training Loss: 0.366581  Accuracy 0.923090  Validation Loss: 0.762468\n",
      "Epoch    37: reducing learning rate of group 0 to 1.1399e-05.\n",
      "Validation loss decreased (0.763093 --> 0.762468). Saving model...\n",
      "Epoch: 38  Training Loss: 0.349406  Accuracy 0.928885  Validation Loss: 0.763292\n",
      "Epoch: 39  Training Loss: 0.354292  Accuracy 0.924144  Validation Loss: 0.771077\n",
      "Epoch    39: reducing learning rate of group 0 to 7.9792e-06.\n",
      "Epoch: 40  Training Loss: 0.363845  Accuracy 0.920457  Validation Loss: 0.768398\n",
      "Epoch: 41  Training Loss: 0.357205  Accuracy 0.925198  Validation Loss: 0.765701\n",
      "Epoch    41: reducing learning rate of group 0 to 5.5855e-06.\n",
      "Epoch: 42  Training Loss: 0.355422  Accuracy 0.928885  Validation Loss: 0.771089\n",
      "Epoch: 43  Training Loss: 0.356369  Accuracy 0.922037  Validation Loss: 0.763069\n",
      "Epoch    43: reducing learning rate of group 0 to 3.9098e-06.\n",
      "Epoch: 44  Training Loss: 0.362038  Accuracy 0.927305  Validation Loss: 0.767384\n",
      "Epoch: 45  Training Loss: 0.363917  Accuracy 0.921510  Validation Loss: 0.768413\n",
      "Epoch    45: reducing learning rate of group 0 to 2.7369e-06.\n",
      "Epoch: 46  Training Loss: 0.353589  Accuracy 0.925724  Validation Loss: 0.763733\n",
      "Epoch: 47  Training Loss: 0.356717  Accuracy 0.922739  Validation Loss: 0.765670\n",
      "Epoch    47: reducing learning rate of group 0 to 1.9158e-06.\n",
      "Epoch: 48  Training Loss: 0.355284  Accuracy 0.923793  Validation Loss: 0.765236\n",
      "Epoch: 49  Training Loss: 0.357613  Accuracy 0.922212  Validation Loss: 0.763958\n",
      "Epoch    49: reducing learning rate of group 0 to 1.3411e-06.\n",
      "Epoch: 50  Training Loss: 0.356620  Accuracy 0.924320  Validation Loss: 0.762146\n",
      "Validation loss decreased (0.762468 --> 0.762146). Saving model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train(n_epochs, loaders, model, optimizer, criterion, use_cuda,save_path):\n",
    "    valid_loss_min = np.Inf\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "\n",
    "        # keep track of training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss=0.0\n",
    "        correct=0.0\n",
    "        # model by default is set to train\n",
    "        for batch_idx, (data, target) in enumerate(loaders['train']):\n",
    "            # move tensors to GPU if CUDA is available\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # update training loss \n",
    "            train_loss += ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
    "            _,pred=torch.max(output,1)\n",
    "            correct+=torch.sum(pred.eq(target.data.view_as(pred)))\n",
    "            \n",
    "            \n",
    "        # validate the model \n",
    "        model.eval()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['valid']):\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            ## update the average validation loss\n",
    "            with torch.no_grad():\n",
    "                output=model(data)\n",
    "            loss=criterion(output.data,target)\n",
    "            #v_total+=data.size(0)\n",
    "            valid_loss += ((1 / (batch_idx + 1)) * (loss.data - valid_loss))\n",
    "        \n",
    "            \n",
    "        # print training/validation statistics \n",
    "        print('Epoch: {}  Training Loss: {:.6f}  Accuracy {:.6f}  Validation Loss: {:.6f}'.format(\n",
    "            epoch, \n",
    "            train_loss,\n",
    "            correct/len(train_data),\n",
    "            valid_loss,\n",
    "            ))\n",
    "        scheduler.step(valid_loss)\n",
    "        #save model\n",
    "        if valid_loss < valid_loss_min:\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}). Saving model...'.format(valid_loss_min, valid_loss))\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            valid_loss_min = valid_loss\n",
    "    \n",
    "    return model\n",
    "\n",
    "loaders={'train':train_Loader,\n",
    "         'valid':valid_Loader,\n",
    "         'test':test_Loader\n",
    "        }\n",
    "model = train(50, loaders, model, optimizer, criterion, use_cuda, 'vgg_model_fine_tune.pt')\n",
    "\n",
    "# load the model that got the best validation accuracy\n",
    "model.load_state_dict(torch.load('vgg_model_fine_tune.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 29102,
     "status": "ok",
     "timestamp": 1582468541560,
     "user": {
      "displayName": "murali perumalla",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mC9CEkZ2SGMnNSmTjQEQaKRd0BHAnwhE1GCElJY=s64",
      "userId": "05154681292036637401"
     },
     "user_tz": -330
    },
    "id": "Jme5q5u_O4su",
    "outputId": "d23cac71-fc27-4c46-c6d1-51b4df5a5308"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.780660 \n",
      " confusion matrix: \n",
      " tensor([[ 82.,   0.,   4.,   1.,   3.,   1.,   1.,   0.],\n",
      "        [  7., 483.,  10., 131.,  33.,  39.,   2.,   7.],\n",
      "        [  9.,   9., 119.,   8.,   2.,   1.,  20.,  18.],\n",
      "        [  3.,  19.,   0.,  83.,   4.,   2.,   0.,   0.],\n",
      "        [  5.,  28.,   3.,  12., 111.,   4.,   0.,   9.],\n",
      "        [  0.,  15.,   7.,   1.,   1.,  78.,   1.,   5.],\n",
      "        [  6.,   1.,  11.,   0.,   1.,   4.,  64.,   6.],\n",
      "        [  4.,   3.,   6.,   0.,   1.,   0.,   1.,  93.]])\n",
      "\n",
      "Test Accuracy: 70% (1113/1582)\n"
     ]
    }
   ],
   "source": [
    "def test(loaders, model, criterion, use_cuda):\n",
    "\n",
    "    # monitor test loss and accuracy\n",
    "    test_loss = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "    confusion_matrix = torch.zeros(8, 8)\n",
    "\n",
    "    model.eval()\n",
    "    for batch_idx, (data, target) in enumerate(loaders['test']):\n",
    "        # move to GPU\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # update average test loss \n",
    "        test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss))\n",
    "        # convert output probabilities to predicted class\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        # compare predictions to true label\n",
    "        correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
    "        total += data.size(0)\n",
    "        for t, p in zip(target.view(-1), pred.view(-1)):\n",
    "                confusion_matrix[t.long(), p.long()] += 1\n",
    "            \n",
    "    print('Test Loss: {:.6f} \\n confusion matrix: \\n {}'.format(test_loss,confusion_matrix))\n",
    "\n",
    "    print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n",
    "        100. * correct / total, correct, total))\n",
    "    return confusion_matrix\n",
    "\n",
    "# call test function    \n",
    "confusion_matrix=test(loaders, model, criterion, use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visualising model using tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xo6vbDvHt77m"
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter('runs/breast_cancer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BK0L1srsmj_Q"
   },
   "outputs": [],
   "source": [
    "class_probs = []\n",
    "class_preds = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data in loaders['test']:\n",
    "        images, labels = data\n",
    "        if use_cuda:\n",
    "          images=images.cuda()\n",
    "        output = model(images)\n",
    "        class_probs_batch = [F.softmax(el, dim=0) for el in output]\n",
    "        _, class_preds_batch = torch.max(output, 1)\n",
    "\n",
    "        class_probs.append(class_probs_batch)\n",
    "        class_preds.append(class_preds_batch)\n",
    "\n",
    "test_probs = torch.cat([torch.stack(batch) for batch in class_probs])\n",
    "test_preds = torch.cat(class_preds)\n",
    "\n",
    "# helper function\n",
    "def add_pr_curve_tensorboard(class_index, test_probs, test_preds, global_step=0):\n",
    "    '''\n",
    "    Takes in a \"class_index\" from 0 to 9 and plots the corresponding\n",
    "    precision-recall curve\n",
    "    '''\n",
    "    tensorboard_preds = test_preds == class_index\n",
    "    tensorboard_probs = test_probs[:, class_index]\n",
    "\n",
    "    writer.add_pr_curve(classes[class_index],\n",
    "                        tensorboard_preds,\n",
    "                        tensorboard_probs,\n",
    "                        global_step=global_step)\n",
    "    writer.close()\n",
    "\n",
    "# plot all the pr curves\n",
    "for i in range(len(classes)):\n",
    "    add_pr_curve_tensorboard(i, test_probs, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 57877,
     "status": "error",
     "timestamp": 1582472627622,
     "user": {
      "displayName": "murali perumalla",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mC9CEkZ2SGMnNSmTjQEQaKRd0BHAnwhE1GCElJY=s64",
      "userId": "05154681292036637401"
     },
     "user_tz": -330
    },
    "id": "B3iDlMsgnrOb",
    "outputId": "7fc0b3d6-a660-4e90-dab1-2896f108f092"
   },
   "outputs": [],
   "source": [
    "!tensorboard --logdir=runs --host localhost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluation metrics\n",
    "\n",
    "sensitivity: TP/(TP+FN) <br>\n",
    "specifity :(TN / (TN+FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 843
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1942,
     "status": "ok",
     "timestamp": 1582473962357,
     "user": {
      "displayName": "murali perumalla",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mC9CEkZ2SGMnNSmTjQEQaKRd0BHAnwhE1GCElJY=s64",
      "userId": "05154681292036637401"
     },
     "user_tz": -330
    },
    "id": "Z3NswkkGySwY",
    "outputId": "bb7b465c-2a9d-4805-cba6-d25cb69d920a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0\n",
      "TP 82.0, TN 1456.0, FP 34.0, FN 10.0\n",
      "Sensitivity = 0.8913043737411499\n",
      "Specificity = 0.9771811962127686\n",
      "Class 1\n",
      "TP 483.0, TN 795.0, FP 75.0, FN 229.0\n",
      "Sensitivity = 0.6783707737922668\n",
      "Specificity = 0.9137930870056152\n",
      "Class 2\n",
      "TP 119.0, TN 1355.0, FP 41.0, FN 67.0\n",
      "Sensitivity = 0.6397849321365356\n",
      "Specificity = 0.9706303477287292\n",
      "Class 3\n",
      "TP 83.0, TN 1318.0, FP 153.0, FN 28.0\n",
      "Sensitivity = 0.7477477192878723\n",
      "Specificity = 0.8959891200065613\n",
      "Class 4\n",
      "TP 111.0, TN 1365.0, FP 45.0, FN 61.0\n",
      "Sensitivity = 0.645348846912384\n",
      "Specificity = 0.9680851101875305\n",
      "Class 5\n",
      "TP 78.0, TN 1423.0, FP 51.0, FN 30.0\n",
      "Sensitivity = 0.7222222089767456\n",
      "Specificity = 0.9654002785682678\n",
      "Class 6\n",
      "TP 64.0, TN 1464.0, FP 25.0, FN 29.0\n",
      "Sensitivity = 0.6881720423698425\n",
      "Specificity = 0.9832102060317993\n",
      "Class 7\n",
      "TP 93.0, TN 1429.0, FP 45.0, FN 15.0\n",
      "Sensitivity = 0.8611111044883728\n",
      "Specificity = 0.9694707989692688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.\n"
     ]
    }
   ],
   "source": [
    "nb_classes=8\n",
    "TP = confusion_matrix.diag()\n",
    "for c in range(nb_classes):\n",
    "    idx = torch.ones(nb_classes).byte()\n",
    "    idx[c] = 0\n",
    "    # all non-class samples classified as non-class\n",
    "    TN = confusion_matrix[idx.nonzero()[:, None], idx.nonzero()].sum() #conf_matrix[idx[:, None], idx].sum() - conf_matrix[idx, c].sum()\n",
    "    # all non-class samples classified as class\n",
    "    FP = confusion_matrix[idx, c].sum()\n",
    "    # all class samples not classified as class\n",
    "    FN = confusion_matrix[c, idx].sum()\n",
    "    sensitivity = (TP[c] / (TP[c]+FN))\n",
    "    specificity = (TN / (TN+FP))\n",
    "\n",
    "    print('Class {}\\nTP {}, TN {}, FP {}, FN {}'.format(\n",
    "            c, TP[c], TN, FP, FN))\n",
    "    print('Sensitivity = {}'.format(sensitivity))\n",
    "    print('Specificity = {}'.format(specificity))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of vgg_fine_tune.ipynb",
   "provenance": [
    {
     "file_id": "1utw58eus7K2JT6PmXnrk_4CV8aVysXzg",
     "timestamp": 1582474517631
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
